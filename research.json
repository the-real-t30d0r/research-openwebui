"""
title:Web Search using SRNXG with BeautifulSoup (Optimized Deep Research)
author: Teodor Cucu (Credits to Jacob DeLacerda for giving me the Idea)
version: 0.1.5
license: MIT
"""

import os
import requests
import json
from urllib.parse import urlparse
import re
import unicodedata
from pydantic import BaseModel, Field
import asyncio
import concurrent.futures
from typing import Any, Callable
from bs4 import BeautifulSoup


class HelpFunctions:
    def get_base_url(self, url: str) -> str:
        url_components = urlparse(url)
        return f"{url_components.scheme}://{url_components.netloc}"

    def generate_excerpt(self, content: str, max_length: int = 200) -> str:
        return content[:max_length] + "..." if len(content) > max_length else content

    def format_text(self, text: str) -> str:
        text = unicodedata.normalize("NFKC", text)
        text = re.sub(r"\s+", " ", text)
        return text.strip()

    def remove_emojis(self, text: str) -> str:
        return "".join(c for c in text if not unicodedata.category(c).startswith("So"))

    def truncate_to_n_words(self, text: str, n: int) -> str:
        words = text.split()
        return " ".join(words[:n])

    def fallback_scrape(self, url_site: str, timeout: int = 20) -> Any:
        try:
            headers = {
                "User-Agent": (
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/90.0.4430.212 Safari/537.36"
                )
            }
            response = requests.get(url_site, headers=headers, timeout=timeout)
            response.raise_for_status()
            html_content = response.text
            soup = BeautifulSoup(html_content, "html.parser")
            for script in soup(["script", "style"]):
                script.extract()
            text = soup.get_text(separator=" ")
            formatted_text = self.format_text(text)
            if len(formatted_text) < 50:
                return None
            return formatted_text
        except Exception:
            return None

    def process_search_result(self, result: dict, valves: Any) -> Any:
        url_site = result.get("url", "")
        if not url_site:
            return None
        if valves.IGNORED_WEBSITES:
            base_url = self.get_base_url(url_site)
            if any(
                ignored.strip() in base_url
                for ignored in valves.IGNORED_WEBSITES.split(",")
            ):
                return None
        content = self.fallback_scrape(url_site)
        if not content:
            content = result.get("content", "")
        if not content or len(content) < 50:
            return None
        return {
            "title": self.remove_emojis(result.get("title", "")),
            "url": url_site,
            "content": self.truncate_to_n_words(
                content, valves.PAGE_CONTENT_WORDS_LIMIT
            ),
            "snippet": self.remove_emojis(result.get("content", "")),
        }


class EventEmitter:
    def __init__(self, event_emitter: Callable[[dict], Any] = None):
        self.event_emitter = event_emitter

    async def emit(self, description="Unknown State", status="in_progress", done=False):
        if self.event_emitter:
            await self.event_emitter(
                {
                    "type": "status",
                    "data": {
                        "status": status,
                        "description": description,
                        "done": done,
                    },
                }
            )


class Tools:
    class Valves(BaseModel):
        SRNXG_API_BASE_URL: str = Field(
            default="http://0.0.0.0:9090", description="Local SearXNG API base URL"
        )
        IGNORED_WEBSITES: str = Field(
            default="", description="Comma-separated list of websites to ignore"
        )
        TOTAL_PAGES_COUNT: int = Field(
            default=30, description="Number of pages to search per query"
        )
        RETURNED_PAGES_COUNT: int = Field(
            default=30, description="Number of pages to return"
        )
        PAGE_CONTENT_WORDS_LIMIT: int = Field(
            default=2000, description="Word limit per page for context"
        )
        CITATION_LINKS: bool = Field(
            default=True, description="Include citation metadata"
        )

    def __init__(self):
        self.valves = self.Valves()
        self.headers = {
            "X-No-Cache": "true",
            "X-With-Images-Summary": "true",
            "X-With-Links-Summary": "true",
        }

    def refine_query(self, topic: str, iteration: int) -> str:
        refine_terms = [
            "detailed analysis",
            "comprehensive review",
            "in-depth insights",
            "extended study",
            "thorough investigation",
        ]
        term = refine_terms[min(iteration, len(refine_terms) - 1)]
        return f"{topic} {term}"

    def generate_report(self, topic: str, results: list) -> str:
        report = f"# Deep Research Report on {topic}\n\n"
        report += "## Mission Outcome and Planning\n"
        report += (
            "A series of iterative internet searches were performed. "
            "For each source, key insights, adjustments to current findings, and missing information were noted. "
            "This process repeated until sufficient data was gathered or 10 iterations were reached.\n\n"
        )
        if not results:
            report += "No relevant sources were found.\n"
            return report
        for idx, result in enumerate(results, start=1):
            report += f"### Source {idx}: {result['title']}\n"
            report += f"**URL:** {result['url']}\n\n"
            report += "#### Key Insights\n"
            report += f"{result['content'][:300]}...\n\n"
            report += "#### Adjustments to Current Findings\n"
            report += "To be determined based on further analysis.\n\n"
            report += "#### Missing Information\n"
            report += "To be determined based on further analysis.\n\n"
        report += "## Citations\n"
        for idx, result in enumerate(results, start=1):
            report += f"{idx}. [{result['title']}]({result['url']})\n"
        return report

    async def search_web(
        self, query: str, __event_emitter__: Callable[[dict], Any] = None
    ) -> str:
        functions = HelpFunctions()
        emitter = EventEmitter(__event_emitter__)
        topic = query
        search_query = query
        await emitter.emit(
            description=f"Internet search initiated for: {topic}. Please wait...",
            status="in_progress",
            done=False,
        )
        all_results = []
        seen_urls = set()
        max_iterations = 5
        min_results_per_iteration = 3
        for iteration in range(max_iterations):
            offset = iteration * self.valves.TOTAL_PAGES_COUNT
            await emitter.emit(
                description=f"Iteration {iteration+1}: Searching for '{search_query}' with offset {offset}...",
                status="in_progress",
                done=False,
            )
            params = {
                "q": search_query,
                "format": "json",
                "number_of_results": self.valves.TOTAL_PAGES_COUNT,
                "offset": offset,
            }
            try:
                response = requests.get(
                    f"{self.valves.SRNXG_API_BASE_URL}/search",
                    params=params,
                    headers=self.headers,
                    timeout=120,
                )
                response.raise_for_status()
            except Exception as e:
                await emitter.emit(
                    description=f"Search error: {str(e)}", status="error", done=True
                )
                break
            search_items = response.json().get("results", [])
            if not search_items:
                await emitter.emit(
                    description="No more search results found. Ending search iterations.",
                    status="in_progress",
                    done=False,
                )
                break
            new_results = []
            with concurrent.futures.ThreadPoolExecutor() as executor:
                futures = [
                    executor.submit(
                        functions.process_search_result,
                        {
                            "title": item.get("title", ""),
                            "url": item.get("url", ""),
                            "content": item.get("snippet", ""),
                        },
                        self.valves,
                    )
                    for item in search_items
                ]
                for future in concurrent.futures.as_completed(futures):
                    res = future.result()
                    if res and res["url"] not in seen_urls:
                        seen_urls.add(res["url"])
                        new_results.append(res)
            if len(new_results) < min_results_per_iteration:
                refined = self.refine_query(topic, iteration)
                await emitter.emit(
                    description=(
                        f"Missing Information: Only {len(new_results)} valid results found. If more information is needed, another internet search is being conducted. Refining query to: '{refined}'."
                    ),
                    status="in_progress",
                    done=False,
                )
                search_query = refined
                continue
            all_results.extend(new_results)
            await emitter.emit(
                description=f"Iteration {iteration+1} added {len(new_results)} valid results (Total: {len(all_results)})",
                status="in_progress",
                done=False,
            )
            if len(all_results) >= self.valves.RETURNED_PAGES_COUNT:
                break
        if not all_results:
            await emitter.emit(
                description="Web search completed. No relevant sources were found.",
                status="complete",
                done=True,
            )
            report = self.generate_report(topic, [])
        else:
            if self.valves.CITATION_LINKS and __event_emitter__:
                for result in all_results:
                    await __event_emitter__(
                        {
                            "type": "citation",
                            "data": {
                                "document": [result["content"]],
                                "metadata": [{"source": result["url"]}],
                                "source": {"name": result["title"]},
                            },
                        }
                    )
                citation_summary = [
                    {"title": r["title"], "url": r["url"]} for r in all_results
                ]
                await __event_emitter__(
                    {
                        "type": "citation_summary",
                        "data": {
                            "message": f"Visited {len(all_results)} websites.",
                            "citations": citation_summary,
                        },
                    }
                )
            await emitter.emit(
                description=f"Web search completed. Retrieved content from {len(all_results)} pages.",
                status="complete",
                done=True,
            )
            report = self.generate_report(topic, all_results)
        return report


async def main():
    async def my_event_handler(event: dict):
        print(
            f"Event: {event['data']['status']} - {event['data']['description']} (Done: {event['data']['done']})"
        )

    tools = Tools()
    report = await tools.search_web("test", __event_emitter__=my_event_handler)
    print("\n--- REPORT ---\n")
    print(report)


if __name__ == "__main__":
    asyncio.run(main())
